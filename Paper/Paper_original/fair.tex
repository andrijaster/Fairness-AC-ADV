%%
%% Copyright 2007-2019 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[T1, T2A]{fontenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{sidecap}
\usepackage{array}
\usepackage{float}
\usepackage[thinlines]{easytable}
\usepackage{fancyhdr}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{adjustbox}
%\usepackage[sc,osf]{mathpazo}%\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{color}
\usepackage{bm}
\usepackage{breakcites}
\usepackage{nomencl}
\makenomenclature
\usepackage{etoolbox}
\usepackage{hyperref}
\usepackage{booktabs}

\renewcommand\nomgroup[1]{%
	\item[\bfseries
	\ifstrequal{#1}{A}{Symbols}{%
		\ifstrequal{#1}{B}{Greek symbols}{%
			\ifstrequal{#1}{C}{Subscripts}{}}}%
	]}
\newcommand{\todelete}[1]{{\bf\textcolor{red}{#1}}}
\newcommand{\added}[1]{{\bf\textcolor{blue}{#1}}}
\newcommand{\replace}[2]{{\bf\textcolor{red}{#1}}{\bf\textcolor{blue}{#2}}}
\newcommand{\mn}[1]{\marginpar{ \scriptsize \textcolor{blue}{MN: #1}}}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}}
%\pagestyle{fancy}
%\fancyhf{}
%\rfoot{\thepage}
%%\geometry{ left=20mm, top=20mm}
\graphicspath{ {Slike/} }
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\def\BState{\State\hskip-\ALG@thistlm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Neurocomputing}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{FAIR: Fair Adversarial Instance Re-weighting}
\tnotetext[label1]{}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author[SIN]{Andrija Petrović\tnoteref{label1}}
\ead{apetrovic@singidunum.ac.rs}
\author[MATF]{Mladen Nikolić}
\author[FON]{Sandro Radovanović}
\author[FON]{Boris Delibašić}
\author[FON]{Miloš Jovanović}

\address[SIN]{Singidunum University - Technical Faculty, Danijelova 32, Belgrade, Serbia}
\address[FON]{University of Belgrade - Faculty of Organizational Sciences, Jove Ilica 154, Belgrade, Serbia}
\address[MATF]{University of Belgrade - Faculty of Mathematics, Studentski Trg 16, Belgrade, Serbia}



\begin{abstract}
With growing awareness of societal impact of artificial intelligence, fairness has become an important aspect of machine learning algorithms. The issue is that human biases towards certain groups of population, defined by sensitive features like race and gender, are introduced to the training data through data collection and labeling. Two important directions of fairness ensuring research have focused on (i) instance weighting in order to decrease the impact of more biased instances and (ii) adversarial training in order to construct data representations informative of the target variable, but uninformative of the sensitive attributes. In this paper we propose a Fair Adversarial Instance Re-weighting (FAIR) method, which uses adversarial training to learn instance weighting function that ensures fair predictions. Merging the two paradigms, it inherits desirable  properties from both interpretability of reweighting and end-to-end trainability of adversarial training. We propose four different variants of the method and, among other things, demonstrate how the method can be cast in a fully probabilistic framework. Additionally, theoretical analysis of FAIR models' properties have been studied extensively. We compare FAIR models to ten other related and state-of-the-art models and demonstrate that FAIR is able to achieve a better trade-off between accuracy and unfairness. To the best of our knowledge, this is the first model that merges reweighting and adversarial approaches by means of a weighting function that can provide interpretable information about fairness of individual instances.
\end{abstract}


% \section*{Research highlights}
%\begin{itemize}
%\item Hybrid model based on combination of two kinds of Gaussian Conditional Random Fields is proposed for traffic state estimation;
%\item Model is intended for prediction of spatially and temporally correlated outputs from sparse data;
%\item The proposed model is tested on two real-world large-scale networks;
%\item Advantages of proposed model are shown by comparison with state-of-the-art unstructured and structured predictors.
%\end{itemize}

\begin{keyword}
Fairness \sep Adversarial training \sep Instance reweighting  \sep Deep learning \sep Classification
\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{Sec:Introduction}
Machine learning algorithms have lead to many recent breakthroughs in different complex tasks that cannot be solved satisfactorily by domain specific algorithms, such as face detection \cite{kumar2019face}, object detection \cite{voulodimos2018deep}, machine translation \cite{singh2017machine}, facial expression recognition
\cite{domadiya2019review}, sport prediction \cite{bunker2019machine}, etc. With this enormous success in practical applications and its growing presence in everyday life, social issues related to machine learning algorithms are becoming increasingly important. One of the most prominent issues is fairness of machine learning algorithms, related to discrimination and bias \cite{hajian2016algorithmic}.

It is well known that in many applications data reflects intended or unintended biases of humans whose actions generated data. Salary prediction \cite{innocenti2016mining}, credit risk prediction \cite{li2019credit}, medical prediction \cite{boyd1996relationship}, personnel planning and recruiting forecasting methods \cite{kim2016data}, are just some of the examples where data, collected from societal interactions, is biased with respect to age, gender, or race. Therefore, machine learning algorithms will extract and learn biases that are present in the data and these can have a strong discriminative impact towards disadvantaged groups.
Improving fairness of biased data and decision procedures based on that data is not only a problem of society, but also a problem of machine learning.  It is critical to guarantee that the prediction obtained by machine learning algorithms is based on appropriate information and that the outcomes are not biased towards certain groups of population defined by sensitive features like race and gender \cite{wang2019approaching}.

Current techniques for improving fairness fall into three different groups: preprocessing techniques \cite{kamiran2012decision,calmon2017optimized}, techniques based on optimization at training time \cite{zafar2019fairness,adel2019one,celis2019classification,kamishima2012fairness}, and post-processing based ones \cite{hardt2016equality, pleiss2017fairness}. State-of-the-art techniques for mitigating bias by preprocessing are based on instance reweighing \cite{kamiran2012data}, a technique that assigns weights to instances as means of controlling their influence on the model during training. The good side of such methods is that weights that the method assigns can be interpreted as indicators of instance fairness. The downside is that the preprocessing procedure is oblivious to
the properties of the downstream learning task, like loss function used, model architecture, etc. That may result in suboptimal weights with respect to that learning task.

Adversarial training has widely been used for finding Nash equilibrium in mini-max (zero-sum) games \cite{goodfellow2014generative, nouiehed2019solving, hsieh2019finding}. Recently, adversarial framework became popular in debiasing deep learning models by introducing two networks, one for predicting output labels and one for predicting sensitive attributes \cite{wadsworth2018achieving, madras2018learning, cevora2020fair, grari2020adversarial}. Both depend on the learnt feature space representation which allows fairly accurate prediction of the output label by the first network, while being maximally uninformative about the sensitive attributes, so that the second network has to fail in its task. While these methods allow for end-to-end training, they do not provide interpretable information on instance fairness, which is desirable.

In this paper, we propose Fair Adversarial Instance Re-weighting (FAIR) -- a novel model for mitigating bias in discriminative dataset by using an adversarial framework to learn an instance reweighing function instead of a new data representation as it is done in previous work. The weighting function can provide interpretable information on instance fairness. Also, FAIR does not perform weighting as preprocessing, but integrates it in the learning procedure so that the learning is performed end-to-end. FAIR consists of three neural networks: the first one is used for determining weights for each instance, the second one for predicting the sensitive attribute, and the third one for predicting the output label. FAIR comes in four variants differing in the weighting method. In the first method (FAIR-scalar), obtained scalar weights are used directly for weighting the log likelihood of corresponding instances, whereas in all other methods instance weights are modelled as random variables parametrized by the weighting network. In the second method (FAIR-Bernoulli) the weights are distributed according to Bernoulli distribution and during learning, score function is used to evaluate the expectation of the log likelihood. The other two methods rely on beta distribution, but they differ in evaluation of the expectation of the log likelihood -- the third one (FAIR-betaSF) uses score function and the fourth one (FAIR-betaREP) relies on reparametrization. Additionally, we discuss how to reduce the variance of FAIR-Bernoulli and FAIR-betaSF using baseline functions. We evaluated our models on four different real-world datasets and compared them to the state-of-the-art techniques. The results demonstrate that FAIR achieved the best results, with respect to fairness and classification performance. Furthermore, to the best of our knowledge, this is the first model that merges reweighting and adversarial approaches relying on a weighting function that can provide interpretable information about fairness of individual instances.

To summaryze, the contributions of this work are as follows:
\begin{itemize}
	\item We merge reweighing and adversarial approaches for mitigating bias in machine learning models, while keeping the best from both.
	\item The proposed method can provide interpretable information about fairness of individual instances.
	\item We provide theoretical analysis of properties of adversarial re-weighting.
	\item We explore several variants of instance weight estimation including probabilistic ones.
	\item We evaluate the method on four different real-world datasets, compared to the state-of-the-art techniques, and provide qualitative analysis.
\end{itemize}

The remainder of the paper is structured as follows. In section~\ref{Sec:Related Work} the related work is reviewed. 
The proposed FAIR algorithm with different variants is described in ~\ref{Sec:FAIR}.
Experimental setup and results on real-world applications are shown in sections ~\ref{Sec:exp-evaluation} and ~\ref{sec:ResultsDiscussion}, respectively. Final conclusions are given in section~\ref{Sec:Conclusion}.


\section{Related Work}
\label{Sec:Related Work}

\textbf{Notion of fairness}. In context of decision-making, (un)fairness has several distinct notions, one of the most prominent being {\em disparate impact} \cite{barocas2016big}. It represents a situation in which decisions ($\hat{\mathbf{y}}$) made by classifier are disproportional between instances with different values of sensitive attributes ($\mathbf{s}$). We use three measures of disparate impact. First metric used is \textit{absolute statistical parity difference}:
\begin{equation}
\mathbf{ASD} = |P(\hat{\mathbf{y}}=1|\mathbf{s}=0) - P(\hat{\mathbf{y}}=1|\mathbf{s}=1)|
\label{metric:asd}
\end{equation}
Low values of $\mathbf{ASD}$ mean that both groups have approximately the same probability of being labeled $1$ (e.g., bank loan granted) by the model. In such case, the classifier is said to have statistical parity.
Second metric we used is \textit{absolute equal opportunity difference}:
\begin{equation}
\mathbf{AEOD} = |TPR_{\mathbf{s}=0} - TPR_{\mathbf{s}=1}|
\label{metric:aeod}
\end{equation}
where $TPR$ represents true positive rate (recall) of the prediction model. Recall reflects opportunity, so this measure can be interpreted as a difference of opportunities between unprivileged and privileged group. Value of AEOD close to 0 is desirable. The third metric that we used is \textit{average odds difference}. Average odds difference can be formulated as:
\begin{equation}
\mathbf{AOD} = \frac{1}{2}(|FPR_{s=unpriv} - FPR_{s=priv}| + |TPR_{s=unpriv} - TPR_{s=priv}|)
\label{metric:aod}
\end{equation}
where $FPR$ represent false positive rate (probability of false alarm), and $TPR$ true positive rate (recall). Values of $\mathbf{AOD}$ close to zero are preferred.

Regarding fairness-aware machine learning algorithms, interested readers are referred to an extensive reviews presented in \cite{friedler2019comparative, bacelar2021monitoring, corbett2018measure, caton2020fairness}. A naive approach to ensuring fairness would be to eliminate sensitive features from the dataset. However, the information contained in the sensitive features can often be approximated from other input features. For example, location of residence correlates with race, although it is not obviously sensitive itself. Therefore, more sophisticated approaches are needed. We focus on two families of methods relevant for our work.

\textbf{Instance Reweighting}. Instance reweighting has shown impressive results, although the idea is relatively simple \cite{feldman2015certifying, krasanakis2018adaptive, ren2018learning, shu2019meta, jiang2018mentornet}. As a preprocessing technique, it was traditionally used for the class imbalance problem by assigning larger weights to instances of a  lower cardinality class, so that the learning algorithm gives more importance to that class. This idea can be applied to the fairness problem as well -- it assigns lower importance to unfair examples or removes them from the learning process. More specifically, those examples will have a lower impact on the likelihood function one tries to optimize.
The simplest approach is to assign weights to instances so that sums of weights per value of a  sensitive feature is the same and all instances from a group have the same weight \cite{kamiran2012data}. That approach was improved in \cite{krasanakis2018adaptive} by utilizing adaptive sensitive reweighting procedure. One can use variational fair auto encoder with Maximum Mean Discrepancy \cite{louizos2015variational} which calculates distances between distributions using kernels. It is worth noticing that instance reweighting has shown to have lower disparate impact \cite{feldman2015certifying} compared to not applying any instance weighting strategy. An advantage of this class of methods is that the weights can be interpreted as indicators of individual instance fairness. However, the training is not end-to-end. To apply some instance reweighting strategy one needs to perform a two step procedure -- first to obtain instance weights, and then to use the weights by the learning algorithm. This is a drawback of this approach since the weighting procedure is oblivious of the model representation and learning algorithm and therefore might choose suboptimal weights for them. 

Recently, Han et. al~\cite{han2021balancing} proposed a very simple but highly effective method for countering bias using instance reweighting, based on the frequency of both task labels and author demographics. Similarly, Hayat~\cite{hayat2021towards} presented two distinct ways for bias mitagation based on reweighting procedure: cluster reweighting method and sample reweighting method. The idea of incorporating an outer optimizer for adaptively selecting mini batch sizes for the purpose of improving model fairness is presented in~\cite{roh2020fairbatch}.

Our approach considerably differs from these approaches in that the weight estimation is integrated in the one step end-to-end learning procedure. Also, in our method the weights are not plain scalars, but the outputs of a neural network. Hence, for purposes of interpretability, such weights can also be estimated for new instances which were not included in the training without retraining the whole system.

\textbf{Adversarial training}. Adversarial training provides a framework for mitigating biases by learning new data representation from which it is possible to predict the target variable, but not possible to predict the sensitive attribute. This approach creates a trade-off between two goal functions and therefore reaches Nash equilibrium \cite{goodfellow2014generative}. Adversarial training can be applied for achieving fairness by generating informative and fair datasets~\cite{rajabi2021tabfairgan, choi2020fair}. The generated fair data can be further used to enhance fair classification performances \cite{jang2021constructing}. The main pros and cons of fair data generation process with generative adversarial networks are presented in~\cite{kenfack2021fairness}.
Adversarial training for directly solving fair classification problem without data generation procedure was first presented in \cite{zhang2018mitigating}. Similar model was applied to recidivism prediction in order to remove racial bias \cite{wadsworth2018achieving, teo2021measuring}.
A theoretical analysis of solving fairness problem via adversarial approach is presented in \cite{madras2018learning}. 
An important approach of such kind is Fair Adversarial Discriminative model (FAD) \cite{adel2019one}. Moreover, theoretical analysis of the relationship between the label classifier performance and the adversary’s ability to predict the sensitive attribute value is provided. Also, in the same paper, a variation of the adversarial learning procedure is developed to increase diversity among elements of each mini-batch of the gradient descent training, in order to achieve a representation that does not suffer from mode collapse. Similarly, Zhao et. al~\cite{zhao2019conditional} presented a algorithm for Conditional Learning of Fair Representations (CLFR) that can simultaneously mitigate two notions of disparity among different subgroups in the classification problems. In addition \cite{celis2021fair} presented an optimization framework to learn fair classifiers in the adversarial setting that comes with provable guarantees on accuracy and fairness.
Another adversarial approach focuses on learning to select non-sensitive features on per instance basis \cite{wang2019approaching}. 
Ragonesi et. al~\cite{ragonesi2021learning} proposed optimization strategy (LURMI - Learning Unbiased Representations via Mutual Information backpropagation), which simultaneously estimates and minimizes the mutual information between the learned representation and specific data attributes by using adversarial framework.
Cotter et. al~\cite{cotter2019optimization} introduced an interesting new approach (PYCO) for solving constrainted optimization problem by introducing Langrangian (PYCO\_diff) and proxy-Lagrangian (PYCO\_non\_diff) based min-max optimization methods for solving differentiable and non-differentiable constraint optimization problems, respectively. It is demonstrated that PYCO can be successfully used for solving group fairness constraint optimization problems. 


The adversarial approach in general is employed to minimize the correlation between selected features and sensitive information. While adversarial approach enables end-to-end training it does not provide any interpretable information on the individual fairness of instances. Our approach differs from these approaches in that it provides interpretable information on instance fairness like reweighting approaches do. That way it tries to keep the best from both worlds.

\section{Fair Adversarial Instance Re-weighting - FAIR}
\label{Sec:FAIR}

The dataset given by $D=\{(\mathbf{x}_i, \mathbf{y}_i, \mathbf{s}_i) \}_{i=1}^N$, consists of input features $\mathbf{x}$, the true label (or the target variable) $\mathbf{y}$ and sensitive features $\mathbf{s}$. It is generated by joint true underlying distribution $D \sim P(\mathbf{x},\mathbf{y},\mathbf{s})$. Unfairness which AI models learn is introduced through data instances containing unfair decisions. Therefore, we strive to recognize if a particular instance in a dataset is unfair. The main principle of FAIR is to reweight log likelihood of each instance, according to the trade-off between fairness and prediction performance, in order to obtain a fair and useful predictor of the target variable.

FAIR consists of three neural networks: the weighting network $f_\theta(\mathbf{x})$, the predictor network $g_\phi(\mathbf{x})$, and the sensitive network $h_\psi(\mathbf{x})$.
For an instance $\mathbf{x}$ the weighting network outputs the weight of that instance $w_\mathbf{x}\in[0,1]$, while the predictor network and the sensitive network output predictions of the output labels $\mathbf{y}$ and the sensitive features $\mathbf{s}$, respectively. We denote probability functions modelled by these networks as $P_\phi(\mathbf{y}|\mathbf{x})$ and $P_\psi(\mathbf{s}|\mathbf{x})$.
In order to incorporate the fairness objective, FAIR weights log likelihood of instances, so that the ones that are strongly informative of the sensitive features, but not of the target variable are assigned low weights and the ones that are informative of the target variable, but not of the sensitive attributes are assigned high weights.
The weighting network is not used during inference, but can be helpful for assessing new instances.

Based on different weighting techniques, we present four different FAIR weighting methods. The first one, FAIR-scalar is based on non-probabilistic weighting framework, whereas FAIR-Bernoulli, FAIR-betaSF, and FAIR-betaREP are based on probabilistic framework. The graphical representation of FAIR with different weighting methods are given in Fig.~\ref{fig:Fig1}.

\begin{figure*}[t!]
	\vskip 0.2in
	\center
	\includegraphics[angle=0, width=1\textwidth, height = 2.7in]{Fig1.pdf}
	\captionsetup{justification=centering}
	\caption{Graphical representations of FAIR with probabalistic and non-probabalistic frameworks}
	\label{fig:Fig1}
	\vskip -0.2in
\end{figure*}

\subsection{FAIR -- non-probablistic framework}
Assume that each instance $\mathbf{x}$ is assigned a scalar weight $f_\theta(\mathbf{x})\in[0,1]$ by a weighting network. Then, FAIR-scalar adversarial problem is given by:
\begin{equation}
\label{Eq:Loss-scalar}
(\theta^*,\phi^*,\psi^*) = \arg\min_{\theta,\phi}\max_{\psi}\mathbb{E}_{\substack{\mathbf{x},\mathbf{y},\mathbf{s} \sim P(\mathbf{x},\mathbf{y},\mathbf{s})\\w=f_\theta(\mathbf{x})}} [w \cdot(\alpha\log P_{\psi}(\mathbf{s}|\mathbf{x})
- \log P_{\phi}(\mathbf{y}|\mathbf{x}))]
\end{equation}
The hyperparameter $\alpha$ controls the trade-off between fairness and predictive performance of the predictor network, but this trade-off will be given further theoretical analysis.


\subsection{FAIR -- probabilistic framework}

In the the case of FAIR with probabilistic approach to weighting, it is assumed that  weights of instances are random variables. 
In contrast to FAIR-scalar, in probabilistic framework, output of the weighting network $f_\theta$ models a probability distribution of instance weights: $P(w_\mathbf{x}|\mathbf{x})$. Consequently, we can use different probability distribution models. Probabilistic approch to $\min-\max$ problems provides guarantees of existence of mixed Nash equilibrium in the cases when pure Nash equilibrium does not exist \cite{tadelis2013game} and could therefore yield better results. We consider Bernoulli (FAIR-Bernoulli) and beta distribution (FAIR-betaSF and FAIR-betaREP). 

FAIR-Bernoulli assumes that log likelihoods of instances, with respect to sensitive features $\log P_{\psi}(\mathbf{s}|\mathbf{x})$ and labels $\log P_{\phi}(\mathbf{y}|\mathbf{x})$ are weighted by integers $w_\mathbf{x} \in \{0,1\}$ such that it holds $P_\theta(w_\mathbf{x}=1|\mathbf{x})=f_\theta(x)$, meaning that the conditional probability of weights is a Bernoulli distribution ${\cal B}(f_\theta(\mathbf{x}))$. The FAIR-Bernoulli adversarial loss ${\cal L}^{\cal B}_{\alpha}(\theta, \phi, \psi)$ is given by:
\begin{gather}
\label{Eq:Loss_prob}
\mathbb{E}_{\substack{\mathbf{x},\mathbf{y}, \mathbf{s} \sim P\left(\mathbf{x},\mathbf{y}, \mathbf{s}\right)\\w \sim P_\theta(w|\mathbf{x})}}  \left[ w \cdot(\alpha\log P_{\psi}\left(\mathbf{s}|\mathbf{x}\right)
-  \log P_{\phi}\left(\mathbf{y}|\mathbf{x}\right))\right]
\end{gather}
and the corresponding adversarial problem is $(\theta^*,\phi^*,\psi^*) = \arg\min_{\theta,\phi}\max_{\psi}{\cal L}^{\cal B}_\alpha(\theta,\phi,\psi)$
where the superscript ${\cal B}$ emphasizes Bernoulli assumption.

In order to optimize the loss, gradients with respect to $\theta$, $\phi$, and $\psi$ need to be computed. Gradients with respect to $\phi$ and $\psi$ are computed by standard backpropagation. However, the gradient with respect to $\theta$ is trickier since $\theta$ defines the distribution of $w$ over which the expectation is taken. Therefore, we derive the gradient of the adversarial loss $\nabla_{\theta}{\cal L}_{\alpha}(\theta, \phi, \psi)$ for FAIR-Bernoulli and FAIR-betaSF as follows:

\begin{equation}
\nabla_{\theta}{\cal L}_{\alpha}(\theta, \phi, \psi) = \nabla_{\theta} \mathbb{E}_{\substack{\mathbf{x},\mathbf{y}, \mathbf{s} \sim P(\mathbf{x},\mathbf{y}, \mathbf{s})\\w \sim P_\theta(w|\mathbf{x})}} \bigg[ w \cdot ( \alpha \log P_{\psi}(\mathbf{s}|\mathbf{x}) - \log P_{\phi}(\mathbf{y}|\mathbf{x}))\bigg]
\end{equation}
The gradient operator $\nabla_{\theta}$ can be propagated through the expectation as:
\begin{align}
\begin{split}
\nabla_{\theta}{\cal L}_{\alpha}(\theta, \phi, \psi) =& \mathbb{E}_{\mathbf{y},\mathbf{x},\mathbf{s}} \bigg[ \int_w \nabla_{\theta}  P_\theta(w|\mathbf{x}) \cdot w \cdot ( \alpha \log P_{\psi}(\mathbf{s}|\mathbf{x}) - \log P_{\phi}\left(\mathbf{y}|\mathbf{x}\right))dw \bigg]
\end{split}
\end{align}
Gradient of the distribution $P_\theta(w|\mathbf{x})$ can be transformed as:
\begin{align}
\begin{split}
\nabla_{\theta}  P_\theta(w|\mathbf{x}) &= P_\theta(w|\mathbf{x}) \cdot \frac{\nabla_{\theta}P_\theta(w|\mathbf{x}) }{ P_\theta(w|\mathbf{x})}
\\
&= P_\theta(w|\mathbf{x}) \cdot \nabla_{\theta} \log P_\theta(w|\mathbf{x})
\end{split}
\end{align}

Following this transformation, the final form of the gradient of the loss with respect to $\theta$ can be represented as:
\begin{align}
\label{Eq:Gradient}
\begin{split}
\mathbb{E}_{\substack{\mathbf{x}, \mathbf{s}, \mathbf{y}\sim P(\mathbf{x},\mathbf{s}, \mathbf{y})\\w \sim P_\theta(w|\mathbf{x})}} \Big[w\cdot \nabla_{\theta}  \log P_\theta(w|\mathbf{x}) \cdot (\alpha\log P_\psi(\mathbf{s}|\mathbf{x}) - \log P_\phi(\mathbf{y}|\mathbf{x})) \Big]
\end{split}
\end{align}
which is a suitable form as it allows the use of the stochastic gradient descent.

Next, we assume that weights $w_\mathbf{x}$ are random variables distributed according to the beta distribution which, in contrast to the case of FAIR-Bernoulli, takes any value from the interval $[0,1]$. The outputs of the weighting network are the parameters $\alpha_\mathbf{x}$ and $\beta_\mathbf{x}$ of the beta distribution. The adversarial loss as defined by Eq.~\ref{Eq:Loss_prob}, but with beta distribution assumed instead of Bernoulli. We denote corresponding loss by ${\cal L}^\beta_\alpha(\theta,\phi,\psi)$ where $\beta$ in the superscript emphasizes the assumed distribution. In optimization, the gradient $\nabla_{\theta}{\cal L}^\beta_{\alpha}(\theta, \phi, \psi)$
can be evaluated either by using score function as in Eq.~\ref{Eq:Gradient} or by the reparametrization trick of beta distribution as shown in \cite{shah2015empirical}. These two approaches we name FAIR-betaSF and FAIR-betaREP respectively.

Pseudocode of probabalistic FAIR with score function (FAIR-Bernoulli and FAIR-betaSF) is presented in Algortihm~\ref{alg:FAIR-sf}. FAIR losses are defined in terms of expectations. However, with finite samples, expectation is always approximated by sample mean, which we use in the algorithm. Incorporation of baseline functions for the reduction of variance of gradient estimate is discussed in~\ref{app:baselines}.

\begin{algorithm}
	\caption{Probabilistic FAIR with score function}
	\label{alg:FAIR-sf}
	\begin{algorithmic}
		\State {\bfseries Input:} learning rates $\gamma_\theta, \gamma_\phi, \gamma_\psi$, dataset $D$,  hyperparameter $\alpha$, probabilistic model ${\cal P}$ of instance weights, number of iterations $M$
		\State {\bfseries Output:} parameters $\theta,\phi,\psi$
		\vspace{2mm}
		\State Initialize $\theta$, $\phi$, $\psi$
		\For{i = 1 to M}
		\State Sample a mini-batch $B\subseteq D$
		\State Sample $w_\mathbf{x}\sim {\cal P}(f_\theta(\mathbf{x}))$ for each $\mathbf{x}$ in $B$
		\State $d_{\theta} \leftarrow \gamma_\theta\frac{1}{|B|} \sum_{(\mathbf{x},\mathbf{y},\mathbf{s})\in B}\left[w_\mathbf{x}\nabla_{\theta}\log P_\theta(w_\mathbf{x}|\mathbf{x})\cdot\right.$
		
		\hspace{3cm}$\left.(\alpha\log P_\psi(\mathbf{s}|\mathbf{x})-\log P_\phi(\mathbf{y}|\mathbf{x}))\right]$
		\State $d_{\phi} \leftarrow \gamma_\phi\nabla_{\phi}{\cal L}^{\cal P}_\alpha(\theta,\phi,\psi,B)$
		\State $d_{\psi} \leftarrow -\gamma_\psi\nabla_{\psi}{\cal L}^{\cal P}_\alpha(\theta,\phi,\psi,B)$
		\State $(\theta,\phi,\psi) \leftarrow (\theta,\phi,\psi) - (d_{\theta}, d_{\phi}, d_{\psi})$
		\EndFor
	\end{algorithmic}
\end{algorithm}



\subsection{Theoretical analysis of model properties}

In order to analyze properties of all our models in a uniform manner, we discuss instance weights as real values in the interval $[0,1]$ and we emphasize dependence of the weight on the instance as $w_\mathbf{x}$ without explicating specifics of the dependence. Vector of all such weights is denoted $\mathbf{w}$ and it is denoted $\mathbf{w}^*$ if it is a part of the optimal solution of the corresponding adversarial problem.
In practice, expectations are approximated by sample means (or sums since outmost constant factors are irrelevant in optimization), and losses are regularized. Therefore we consider a regularized loss ${\cal L}_{\alpha}(\mathbf{w}, \phi, \psi)$:
\begin{equation}
\label{Eq:Loss-scalar-new}
\begin{split}
\sum_{(\mathbf{x},\mathbf{y},\mathbf{s})\in D} w_\mathbf{x} [\alpha\log P_{\psi}(\mathbf{s}|\mathbf{x}) -  \log P_{\phi}(\mathbf{y}|\mathbf{x})]\\
\text{s.t. }\|\theta\|^2_2+\|\phi\|^2_2+\|\psi\|^2_2\leq \lambda
\end{split}
\end{equation}
where dependence of $w_\mathbf{x}$ on $\theta$ is not made explicit, but we are aware that it exists.
To shorten the proofs, we formulate regularization in a constraint based manner \cite{tibshirani1996regression}, although it is more often formulated and implemented in a mathematically equivalent penalty based manner (note that the meaning of regularization parameter is reversed -- in penalty based formulation case $\lambda=0$ corresponds to an infinite value of $\lambda$ in constraint based formulation).

Now we theoretically analyze the behaviour of our method. The focus of the analysis is on the following elements:
\begin{itemize}
	\item the instance weights,
	\item the predictive performance of predictive and sensitive networks for a specific instance as measured by $\log P_{\phi}(\mathbf{y}|\mathbf{x})$ and $\log P_{\psi}(\mathbf{s}|\mathbf{x})$, which are related to instance predictive quality and its fairness,
	\item the hyperparameter $\alpha$, 
\end{itemize}
and their interdependence. The interdependence is most easily observed with respect to variation of the hyperparameter $\alpha$, which is under direct control of the user. However, the interaction of all elements is relevant for the understanding of the method. First, we focus on how the variation of the hyperparamter reflects on the trade-off between fairness and the quality of prediction of the target variable. In a nutshell, extreme case $\alpha=0$ represents extreme emphasis on fairness and $\alpha\rightarrow\infty$ represents extreme emphasis on quality of prediction and disregard for fairness. Please note that a superficial glance at the adversarial problem would suggest vice versa, but we stress that it is not the case. Second, we aim to understand how the hyperparameter $\alpha$ affects the optimal weights assigned to the instances. It turns out that under some (reasonable) conditions the optimal weights will tend to $0$ and $1$ and that the value of $\alpha$ controls the proportion of the two limiting values. The effect of $\alpha$ on the weights and thus on model behvaiour is mediated by the ratio of performance of predictive and sensitive networks. These elements of the formal analysis also provide better intuitive understanding of the method. We provide such discussion after the theoretical results.

\begin{lemma}
	If $\lambda$ is finite, there exist strictly negative constants
	$c_\phi$, $c'_\phi$, $c_\psi$, and $c'_\psi$ such that it holds
	$c_\phi\leq \log P_\phi(\mathbf{y}|\mathbf{x})\leq c'_\phi$ and
	$c_\psi\leq \log P_\phi(\mathbf{s}|\mathbf{x})\leq c'_\psi$ for
	any $\mathbf{x}$, $\mathbf{y}$, and $\mathbf{s}$, and any
	$\phi$ and $\psi$ which satisfy regularization condition \ref{Eq:Loss-scalar-new}.
	\label{pp:bounded}
\end{lemma}
\begin{proof}
	Denote ${\cal B}$ the ball defined by $\|\theta\|^2_2+\|\phi\|^2_2+\|\psi\|^2_2\leq \lambda$, representing the set of feasible solutions of the optimization problem.
	Denote $\bar{g}_\phi(\mathbf{x})$ the network $g_\phi(\mathbf{x})$ modelling $\mathbf{y}$ with sigmoid function at the output removed and $\bar{h}_\psi(\mathbf{x})$ the network $h_\psi(\mathbf{x})$ modelling $\mathbf{s}$ with sigmoid at the output removed. Since ${\cal B}$ is a compact set and $\bar{g}_\phi(\mathbf{x})$ and $\bar{h}_\psi(\mathbf{x})$ are continuous functions, they both attain their finite minimal and maximal values within ${\cal B}$. Since $\log P_\psi(\mathbf{s}|\mathbf{x})$ and $\log P_\phi(\mathbf{y}|\mathbf{x})$ are continuous functions of  $\bar{h}_\psi(\mathbf{x})$ and $\bar{g}_\phi(\mathbf{x})$, respectively, which map the range of $\bar{h}_\psi$ and $\bar{g}_\phi$ from $(-\infty,\infty)$ to $(-\infty, 0)$, functions $\log P_\psi(\mathbf{s}|\mathbf{x})$ and $\log P_\phi(\mathbf{y}|\mathbf{x})$ attain their strictly negative and finite minimal and maximal values within ${\cal B}$. Therefore, the required constants exist, by which the lemma is proven.
\end{proof}

\begin{theorem}
	If $\lambda$ is finite, for $\alpha=0$ it holds $\mathbf{w}^*=\mathbf{0}$.
	\label{pp:zero}
\end{theorem}
\begin{proof}
	By Lemma \ref{pp:bounded}, $P_{\psi}(\mathbf{s}|\mathbf{x})$ is bounded, so for $\alpha=0$ it holds:
	\begin{align}
	\begin{split}
	(\mathbf{w}^*,\phi^*,\psi^*)&=\arg\min_{\mathbf{w},\phi}\max_{\psi}{\cal L}_\alpha(\mathbf{w},\phi,\psi)\\
	&=\arg\min_{\mathbf{w},\phi}- \sum_{(\mathbf{x},\mathbf{y},\mathbf{s})\in D} w_\mathbf{x} \cdot \log P_{\phi}(\mathbf{y}|\mathbf{x})
	\end{split}
	\end{align}
	By Lema \ref{pp:bounded}, $\log P_{\phi}(\mathbf{y}|\mathbf{x})$ is strictly negative, so $- \sum_{(\mathbf{x},\mathbf{y},\mathbf{s})\in D} w_\mathbf{x} \cdot \log P_{\phi}(\mathbf{y}|\mathbf{x})$ is zero or positive. Therefore its minimal value is $0$ for $\mathbf{w}=\mathbf{0}$ regardless of $\phi$.
	Therefore, it holds $\mathbf{w}^*=\mathbf{0}$.
\end{proof}

\begin{theorem}
	For each instance $(\mathbf{x},\mathbf{y},\mathbf{s})$, it holds
	$w_{\mathbf{x}}^*= 1$ or $w_{\mathbf{x}}^*= 0$ or $\alpha\log P_{\psi^*}(\mathbf{s}|\mathbf{x})=\log P_{\phi^*}(\mathbf{y}|\mathbf{x})$.
	\label{pp:anypoint}
\end{theorem}
\iffalse
\begin{proof}
	Consider a partial derivative of ${\cal L}_\alpha$ for fixed but arbitrary $\phi$ and $\psi$:
	$$\frac{\partial{\cal L}_\alpha}{\partial w_\mathbf{x}}(\mathbf{w},\phi,\psi)=\alpha\log P_{\psi}(\mathbf{s}|\mathbf{x})-\log P_{\phi}(\mathbf{y}|\mathbf{x})$$
	This derivative can be equal to $0$ (in which case it holds $\alpha\log P_{\psi}(\mathbf{s}|\mathbf{x})=\log P_{\phi}(\mathbf{y}|\mathbf{x})$), negative, or positive.
	If the derivative is negative, for those fixed $\phi$ and $\psi$, ${\cal L}_\alpha(\mathbf{w},\phi,\psi)$ is a strictly decreasing function of $w_\mathbf{x}$, so its minimum is attained for $w_\mathbf{x}=1$. If this derivative is positive, minimal value is attained for $w_\mathbf{x}=0$. Therefore, for arbitrary fixed $\phi$ and $\psi$, it either holds $\alpha\log P_{\psi}(\mathbf{s}|\mathbf{x})=\log P_{\phi}(\mathbf{y}|\mathbf{x})$ or it holds that the minimum is attained for $w_\mathbf{x}=1$ or for $w_\mathbf{x}=0$. Since this holds for arbitrary $\phi$ and $\psi$, it also holds for optimal choices $\phi^*$, $\psi^*$, by which the theorem is proven.
\end{proof}
\fi

\begin{proof}
	Consider a partial derivative in the optimal solution:
	\begin{equation}
	\frac{\partial{\cal L}_\alpha}{\partial w_\mathbf{x}}(\mathbf{w}^*,\phi^*,\psi^*)=\alpha\log P_{\psi^*}(\mathbf{s}|\mathbf{x})-\log P_{\phi^*}(\mathbf{y}|\mathbf{x})
	\end{equation}
	If the derivative is negative, then there exists $d>0$ such that it holds
	\begin{equation}{\cal L}_\alpha(\mathbf{w}^*+d\mathbf{e}_{\mathbf{x}},\phi^*,\psi^*) < {\cal L}_\alpha(\mathbf{w}^*,\phi^*,\psi^*)
	\end{equation}
	where $\mathbf{e}_\mathbf{x}=(0,\ldots,1,\ldots,0)\in\mathbb{R}^{|D|}$ where $1$ is at the coordinate corresponding to $w_\mathbf{x}$.
	Therefore, if it holds $w_\mathbf{x}^*< 1$, $w_\mathbf{x}^*$ can be increased in order to decrease the loss and $(\mathbf{w}^*,\phi^*,\psi^*)$ is not an optimal solution, which is a contradiction. Therefore, it has to hold $w_\mathbf{x}^*=1$.
	If the derivative is positive, $w_\mathbf{x}^*=0$ is proven in an analogous manner.
	If the derivative is $0$, the theorem holds due to its third case.
\end{proof}


In the following propositions, we explicitly denote dependence of the optimal solution on $\alpha$.

\begin{lemma}
	If $\lambda$ is finite, for each instance $(\mathbf{x},\mathbf{y},\mathbf{s})\in D$ it holds
	\begin{equation}
	\frac{\partial{\cal L}_\alpha}{\partial w_\mathbf{x}}(\mathbf{w}^*_\alpha,\phi^*_\alpha,\psi^*_\alpha)\rightarrow -\infty\hspace{5mm}\text{as}\hspace{5mm}\alpha\rightarrow\infty
	\end{equation}
	\label{pp:regularization}
\end{lemma}
\begin{proof}
	Consider a partial derivative with respect to $w_\mathbf{x}$ in an optimum:
	\begin{equation}
	\frac{\partial{\cal L}_\alpha}{\partial w_\mathbf{x}}(\mathbf{w}^*_\alpha,\phi^*_\alpha,\psi^*_\alpha)=\alpha\log P_{\psi^*_\alpha}(\mathbf{s}|\mathbf{x})-\log P_{\phi^*_\alpha}(\mathbf{y}|\mathbf{x})
	\end{equation}
	According to Lemma \ref{pp:bounded}, for any feasible $\psi$ and $\phi$ there exists constants $c'_\psi<0$ and $c_\phi$ such that it holds $\log P_{\psi}(\mathbf{s}|\mathbf{x})\leq c'_\psi$ and $\log P_{\phi}(\mathbf{y}|\mathbf{x})\geq c_\phi$. Therefore, the first term goes to $-\infty$ as $\alpha\rightarrow\infty$ and the second term is bounded, so the limit of the partial derivative is $-\infty$.
\end{proof}

\begin{theorem}
	If $\lambda$ is finite, for each instance $(\mathbf{x},\mathbf{y},\mathbf{s})\in D$, it holds $w_{\mathbf{x},\alpha}^*\rightarrow 1$ as $\alpha\rightarrow\infty$.
	\label{pp:infty}
\end{theorem}
\begin{proof}
	According to Lemma \ref{pp:regularization},
	the limit of the values of the partial derivative $\frac{\partial{\cal L}_\alpha}{\partial w_\mathbf{x}}(\mathbf{w}^*_\alpha,\phi^*_\alpha,\psi^*_\alpha)$ in optima as $\alpha\rightarrow\infty$ is negative. Then, by the definition of the limit, there exists $\alpha_0\in \mathbb{R}$ such that for all $\alpha>\alpha_0$ it holds:
	\begin{equation}
	\frac{\partial{\cal L}_\alpha}{\partial w_\mathbf{x}}(\mathbf{w}^*_\alpha,\phi^*_\alpha,\psi^*_\alpha)<0
	\end{equation}
	For each such $\alpha$, since derivative with respect to $w_\mathbf{x}$ is negative, by the same argument as in the proof of Theorem \ref{pp:anypoint}, it holds $w^*_{\mathbf{x},\alpha}=1$. Hence, we can conclude that for each $\varepsilon>0$, there exists $\alpha_0$ such that for all $\alpha>\alpha_0$ it holds $w^*_{\mathbf{x},\alpha}>1-\varepsilon$  (since $w^*_{\mathbf{x},\alpha}=1$). Therefore, by the definition of the limit, we conclude that it holds $w^*_{\mathbf{x},\alpha}\rightarrow 1$ as $\alpha\rightarrow\infty$.
\end{proof}

\subsection{Intuitive discussion of theoretical properties}

Provided theorems explain the way the crutial elements of the model interact when set in motion by varying the hyperparameter $\alpha$. It turns out that the hyperparameter can be understood as a threshold on the ratio of instance ``predictiveness'' and instance fairness based on which the model decides if the instance should be discarded or exploited in learning.
If it holds:
\begin{equation}
\frac{\log P_{\phi}(\mathbf{y}|\mathbf{x})}{\log P_{\psi}(\mathbf{s}|\mathbf{x})}<\alpha
\end{equation}
intuitively, the instance is fair enough considering its ``predictiveness'' (with respect to the target variable). Namely, for the ratio to be low, its ``predictiveness'' should be high (reflected by small negative value of log likelihood in the numerator) and its unfairness should be low (reflected by the large negative value of log likelihood in the denominator). In the extreme case of $\alpha=0$ no instance is considered fair enough, since neither the log likelihood in the numerator can be exactly zero, nor the log likelihood in the denominator can be infinite. According to Theorem \ref{pp:zero}, in that case, all instances are discarded. In the other extreme, according to Theorem \ref{pp:infty}, as $\alpha$ tends to infinity, fairness is disregarded and all instances are used for learning. For values of $\alpha$ in between some instances are disregarded and some are used. Note that this insight is not only about the hyperparameter $\alpha$, but also about the way the model judges predictive qualities of the instance and their trade-off and how, based on that, it weights them differently.

Besides this central insight, other aspects of the tehoretical analysis merit a comment. In case of infinite $\lambda$, overfitting might falsify our proof of Lemma \ref{pp:bounded} and in that case for some instance $\mathbf{x}$ it might hold $w^*_{\mathbf{x},\alpha}\rightarrow 0$ as $\alpha\rightarrow\infty$. However, this suggests an interesting diagnostic property -- if for ever larger values of $\alpha$ one obtains $w_\mathbf{x}=0$ for some $\mathbf{x}$, one has reasons to suspect overfitting. Also, finite capacity of the network might make regularization unnecessary in practice. However, theoretical analysis was easier under the assumption of explicit regularization.

Also note that the model of instance weights does not need allow values $0$ and $1$. Nevertheless, the provided theorems inform us that the gradients will push the weights towards these values. Still, our probabilistic approaches might provide additional regularization by giving nonzero probability to other weight values except the optimal ones.



\section{Experimental Setup}
\label{Sec:exp-evaluation}

\textbf{Datasets}. The proposed framework was tested on four datasets, three of which are commonly used benchmarks. Two datasets (German credit and Adult income) come from the UCI ML repository \cite{frank2011uci}. To our knowledge the Hospital readmission dataset was used in this paper for the first time in the context of fairness.

The first, the \textit{Adult income} dataset \cite{kohavi1996scaling} represents a binary classification task of predicting whether an income is greater than 50K dollars. The dataset contains 45,222 instances described by 14 features and including the sensitive attribute Gender. The atributes used in the dataset describes the individual's education level, age, gender, occupation, workclass, martial-status, relationship, capital loss and etc \cite{Dua:2019}. After applying dummy coding, total number of features was 93. Total numbers of instances used in training, validation and testing are 31,655, 6,783, and 6,784, respectively.

Second dataset we used is the \textit{Hospital readmission} dataset \cite{stiglic2015comprehensible}. It represents a binary classification task where label 1 means that patient is readmitted within 30 days. The dataset consists of 66,994 instances and 931 attributes, including sensitive attribute Gender. Total number of instances used in training, validation and testing are 46,895, 10,049 and 10,050, respectively.

The third dataset, named \textit{Hospital Expenditures}, comes from \cite{bellamy2019ai}. It represents a binary classification task of predicting whether a person would have high or low utilization of medical expenditures. The sensitive attribute is Race. Dataset contains 15,830 instances and 133 attributes, after dummy codding, total number of attributes used in this dataset was 138. For training, validation, and testing, we used 11,081, 2,374 and 2,375 instances respectively.

As a fourth dataset, we used \textit{German credit} dataset. \textit{German credit} dataset has 1,000 instances where the task is to classify bank account holders into classes good or bad. The total number of attributes used in the dataset, after applying dummy coding is 58, including sensitive attributes. Following the definition of fairness from \cite{kamiran2012decision} for German credit dataset, there are two sensitive attributes, one being Gender and other being Age ($\geq 25$ is considered as privileged class, and $< 25$ as unprivileged class).  Total numbers of instances used in training, validation and testing was 700, 150, and 150, respectively.

\textbf{Models}. \sloppy The results obtained by FAIR models are compared with ten related and state-of-the-art algorithms: FAD, reweighing preprocessing technique from~\cite{kamiran2012data} combined with the random forest classifier (Reweighing - RF) and with neural networks (Reweighing - NN), disparity impact remover~\cite{feldman2015certifying} combined with random forest (DI - RF) and neural networks (DI - NN), prejudice remover~\cite{kamishima2012fairness} (PR), models based on conditonal learning of fair representations~\cite{zhao2019conditional} (CLFR), learning unbiased representation via mutual information~\cite{ragonesi2021learning} (LURMI), optimization model for differentiable and non-differentiable constraints~\cite{cotter2019optimization} (PYCO\_diff and PYCO\_non\_diff). In the case of PYCO\_non\_diff model, all fairness metrics were directly optimized, whereas in the case of PYCO\_diff, approximation of fairness constraints is used. Architecture, number of epochs in early stopping procedure, and learning rates were empirically determined as to optimize the performance of each model, by varying design choices of the architectures described in the literature. Detailed specifications can be found in \ref{app:Architecture}. We did not use explicit regularization in our experiments since the capacity of the models can also be controlled through the choice of architecture and early stopping.

\textbf{Optimization}.
For optimization of all neural network based models we use Adam optimizer \cite{bock2019proof}.
During optimization of FAIR and FAD models, early stopping was used. In the early stopping procedure, the min and max objectives of adversarial training procedure on validation set were monitored. In case when there were no improvements in either of these two metrics for a given number of epoch (provided in \ref{app:Architecture}), the training procedure is stopped.

\textbf{Metrics}.
Classification performance of all presented classifiers is quantified by the accuracy (ACC), which is calculated for the target variable ($\mathbf{y}$) and the sensitive attribute ($\mathbf{s}$). Therefore, we present ACC$_\mathbf{y}$ and ACC$_\mathbf{s}$ for the target variable and the sensitive attribute, respectively. If subscript is omitted, then ACC$_\mathbf{y}$ is presented. As fairness metrics we use ASD, AEOD, and AOD defined by Eqs. \ref{metric:asd}, \ref{metric:aeod}, and \ref{metric:aod}, respectively. 

\textbf{Evaluation procedure and presentation of results}. The evaluated models (both FAIR and the baselines) have hyperparameters which affect the trade-off between fairness and predictive performance of the classifiers. Note that such hyperparameters do not control model capacity. Therefore, we do not tune them to obtain maximal perfomance (like one might tune regularization hyperparameters). Instead, we vary them in order to illustrate model behaviour for different trade-offs. The hyperparameters $\alpha$ of FAIR, CLFR, LURMI and FAD models were varied in range $[0, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 10^{2}, 10^{3}]$, whereas in the case of other models, hyperparameters were varied in range $[0, 10^{-3}, 10^{-2}, 10^{-1}, 1]$ (since $1$ is the maximal value for these methods). For each such value, evlauation metrics were computed. PYCO\_diff and PYCO\_non\_diff models were optimized for each fairness metric separately. We call a set of models obtained from one model kind (FAD, FAIR, etc.) by varying the fairnes related hyperparamter, a {\em model family}. For instance, all FAD models trained for different values of $\alpha$ constitute a FAD model family.

Since the models are evaluated by two criteria (predictive performance of the target variable and fairness), one model can be better than the other according to one criterion and vice-versa. Since both critera are important, instead of privileging one of them we present our results in terms of Pareto fronts. For a set of trained models, Pareto front consists of models which are not dominated by any other models in terms of both predictive performance and fairness \cite{marler2004survey}. Models which are dominated by others in terms of both criteria are obviously irrelevant and should be discarded. Pareto front can be plotted in 2D in terms of metrics for the two criteria used and visually inspected. We base our evaluation on the overall Pareto front which is a Pareto front of all trained models (union of all model familites). Models which yield more points in such Pareto front are better.
Since FAIR and baseline models do not directly optimize any of the commonly used fairness metrics, we evaluate them using several different fairness metrics. Hence, in~\ref{app:Additional_results} we present the overall Pareto front of all trained models with respect to AOD, ASD, and AEOD as fairness metrics and ACC as the performance metric.

Construction of the Pareto front includes model selection - models are compared according to their performance and some of them are selected. Since evaluation metrics should never be reported on the data on which the selection was performed, we take care to train all models on the training set, to perform selection of the models for the Pareto front on the validation set, and to evaluate selected models on the test set. All results reported in the following section are calculated on the test set.

\section{Results and Discussion}
\label{sec:ResultsDiscussion}

In this section we provide experimental results following the above described setup. Further on, we provide the discussion of these results and the qualitative evaluation of the behaviour of our model.

\subsection{Results}
\label{sec:Results}

In this section we present results obtained using the experimental evaluation outlined above.



Firstly, model performances obtained on the \textit{Adult income} dataset are illustrated in Fig.~\ref{fig:Adult} by three fairness metrics (AOD, ASD or AEOD) and classification performance (ACC$_\mathbf{y}$). The models with greater ACC score and lower (un)fairness metric (upper left corner of plots) are preferred. It can be observed that FAIR models dominates Pareto optimal solutions with respect to the all fairness metrics. In addition, FAIR-beta and LURMI models dominate the upper left corner of Pareto fronts for AOD and AEOD, whereas the FAIR-scalar and PYCO\_non\_diff dominate the upper left corner of Pareto front for ASD metric. 

\begin{figure*}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{Adult_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Adult income} datasets}
	\label{fig:Adult}
	\vskip -0.2in
\end{figure*}

Secondly, the results obtained on the \textit{Hospital readmission} dataset are presented in Fig.~\ref{fig:Readmission}. It can be noticed that only FAIR models exist on Pareto front and consequently all other models are dominated by them. It can be observed that in the case of all three fairness metrics FAIR-betaSF is the closest to the upper left corner and can therefore be considered better than others. 

\begin{figure}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{Readmission_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Hospital readmission} dataset}
	\label{fig:Readmission}
	\vskip -0.2in
\end{figure}

Thirdly, models performances obtained on the \textit{Hospital expenditures} dataset are illustrated in Fig.~\ref{fig:MEPS19}. It can be observed that the FAIR models dominate Pareto front in all presented metrics. In the case of ASD metric FAIR-scalar and PYCO models are the closest to the upper left corner of Pareto front, whereas in the case of AOD metric FAIR-betaSF is the best one and in the case of AEOD metrics it is the PYCO\_non\_diff model. 

\begin{figure}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{MEPS19_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Hospital expenditures} dataset}
	\label{fig:MEPS19}
	\vskip -0.2in
\end{figure}



Eventually, model performances obtained of \textit{German credit} dataset for age and sex as sensitive attributes are presented in Figs.~\ref{fig:Ger-age-res} and~\ref{fig:Ger-sex-res}, respectively. It can be observed that in  Fig.~\ref{fig:Ger-sex-res} all models are equally represented, whereas in  Fig.~\ref{fig:Ger-age-res} FAIR models dominate the Pareto front in all cases. 
%In Table~\ref{tab:Ger-age} it can be observed that all models are equally represented in Pareto fronts, whereas in Table~\ref{tab:Ger-sex} the most dominant models are Reweighing-RF and FAD.

\begin{figure}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{Ger_age_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{German credit} (age) dataset}
	\label{fig:Ger-sex-res}
	\vskip -0.2in
\end{figure}

\begin{figure}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{Ger_sex_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{German credit} (sex) dataset}
	\label{fig:Ger-age-res}
	\vskip -0.2in
\end{figure}


Additional results can be found in~\ref{app:Additional_results}. 

\subsection{Discussion and qualitative study}

To summarize, FAIR variants often perform better than other methods, but of course the other methods can outperform them. Therefore we conclude that FAIR is roughly equal or better to the state-of-the-art methods. Most notable baselines seem to be PYCO methods. Among the FAIR variants, FAIR-beta seems to be most promising in terms of discussed accuracy and fairness metrics.
A relevant question regarding FAIR models is which variant shows most promise and which one should be used in practice. The reason for introduction of probabilistic variants was the guaranteed existence of mixed Nash equilibrium, which is important since the existence of pure Nash equilibrium is not guaranteed for $\min-\max$ problems and therefore FAIR-scalar might not converge to a mathematically meaningful solution \cite{tadelis2013game}. The fact that FAIR-beta achieves somewhat better performance might be caused by such issue. On the other hand, the the advantage of FAIR-beta is not drastic and the simplicity of FAIR-scalar might make it a more appealing approach in practice if one can tolerate small drops in performance metrics. 

Model behaviour of FAIR model with respect to change of hyperparameter $\alpha$ is shown in Fig.~\ref{fig:FigResultsAlpha} on the \textit{Geman credit} dataset. It can be observed that as $\alpha$ decreases, instances which are unfair (but potentially useful for prediction of target variable) are being discarded, so ACC metrics for both the target variable and sensitive attribute decrease. This is experimental verification of theoretical model properties presented in section~\ref{Sec:FAIR}.

\begin{figure}[h]
	\center
	\includegraphics[angle=0, width=0.8\textwidth]{AUC_y_A.png}
	\captionsetup{justification=centering}
	\caption{ACC$_\mathbf{y}$ and ACC$_\mathbf{s}$ as functions of the fariness hyperparameter $\alpha$ measured on the German credit - sex and Readmission datasets (ACC$_\mathbf{y}$ is preferred larger, and ACC$_\mathbf{s}$ smaller)}
	\label{fig:FigResultsAlpha}
\end{figure}

Furthermore, we increased the hyperparameter $\alpha$ in FAIR-scalar model from 0 to the first value where one of the instance in training dataset has weight that tends to 1. Based on theoretical formulation of model properties this is the most fair instance in dataset. Moreover, we kept to increase parameter $\alpha$ until the first two instances with weights that tends to 1 in opposite sex and label categories occurred. In Table~\ref{tab:Explainability} the attributes of previously mentioned instances are presented.

Firstly, it could be observed that the most fair instance has good credit score mainly based on facts that he is employed as manager, does not have other debtors and credits taken, posses life insurance and house, is not a foreign worker, has small amount of money on checking account. Similar, attributes can be seen in the case of the first ``fair'' instance with good credit score that is female. She is not a foreign worker, employed as manager for 7 or more years, paid back duly existing credits and took credit for buying new car. She has small amount of money on checking account and does not have other debtors. Unlike this two instances, the first instance with bad credit score is unemployed man, that has other debtors, is foreign worker, does have house and car. It is obviously that unemployment and other debts has the most influential impact on labelling this instance as bad.

It can be concluded that all presented instances have reasonable explanations why they are labelled with bad or good credit score. Furthermore, it can be seen that sex does not have any kind of cause on final decision so FAIR-scalar successfully labelled them as fair.

\begin{table}
	\centering
	\caption{German credit dataset instances with non zero weights}
	\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Credit   duration}                               & 48                         & 36                                                                                   & 36                                                                                   \\ \hline
		\textbf{Credit amount}                                   & 18424                      & 14318                                                                                & 15857                                                                                \\ \hline
		\textbf{Investment as income percentage}                 & 1                          & 4                                                                                    & 2                                                                                    \\ \hline
		\textbf{Residence since}                                 & 2                          & 2                                                                                    & 3                                                                                    \\ \hline
		\textbf{No.of credits taken}                             & 1                          & 1                                                                                    & 1                                                                                    \\ \hline
		\textbf{No. of people liable to provide maintenance for} & 1                          & 1                                                                                    & 1                                                                                    \\ \hline
		\textbf{Status of checking account}                      & \textless{}200 DM          & \textless{}200 DM                                                                    & \textless{}0DM                                                                       \\ \hline
		\textbf{Credit history}                                  & no   credits taken         & existing credits paid back duly till now &  existing credits paid back duly till now \\ \hline
		\textbf{Purpose}                                         & other                      & car (new)                                                                            & other                                                                                \\ \hline
		\textbf{Savings}                                         & \textless{}100 DM          & \textless{}100 DM                                                                    & \textless{}100 DM                                                                    \\ \hline
		\textbf{Employment}                                      & 1\textless{}4 years        & \textgreater{}=7 years                                                               & unemployed                                                                           \\ \hline
		\textbf{Other debtors}                                   & none                       & none                                                                                 & co-applicant                                                                         \\ \hline
		\textbf{Properties}                                      & Life insurance             & unknown                                                                              & car or other                                                                         \\ \hline
		\textbf{Installment plans}                               & bank                       & none                                                                                 & none                                                                                 \\ \hline
		\textbf{Housing}                                         & own                        & for free                                                                             & own                                                                                  \\ \hline
		\textbf{Skill level}                                     & management                 & management                                                                           & self-employed                                                                        \\ \hline
		\textbf{Telphone}                                        & yes,   under customer name & yes,   under customer name                                                           & none                                                                                 \\ \hline
		\textbf{Foreign worker}                                  & no                         & no                                                                                   & yes                                                                                  \\ \hline
		\textbf{Sensitive attribute - Sex}                                             & male                       & female                                                                               & male                                                                                 \\ \hline
		\textbf{Credit score -Label}                                           & Good                       & Good                                                                                 & Bad                                                                                  \\ \hline
	\end{tabular}}
	\label{tab:Explainability}
\end{table}

\section{Conclusions}
\label{Sec:Conclusion}
We introduced a Fair Adversarial Instance Re-weighting (FAIR) discriminative method, which uses adversarial training to learn instance weights to ensure fairness. We proposed four different variants of the method: a non probabilistic one and three models cast in fully probabilistic framework. In addition, we presented a possibility to introduce a baseline to reduce variance of gradient estimation for models based on score function. Theoretical analysis of FAIR model behaviour in terms of interactions of its main elements is given. We explained how changing the value of the hyperparameter controls the trade-off between model fairness and predictive performance. In experimental evaluation on five real-world tasks we demonstrated that our models are better or equal to previous state-of-the-art approaches with respect to fairness metrics and classification performance. Its additional desirable feature is its ability to provide interpretable information on the individual fairness of instances which existing adversarial approaches do not provide. Moreover, in the qualitative study, we demonstrated that FAIR model is able to find ``fair'' instances for small values of the hyperparameter $\alpha$.

Further studies should address extending FAIR models to numerical and categorical values of sensitive attributes and adding additional loss constraints for individual fairness.

\section*{Acknowledgement}
This work was supported in part by the ONR/ONR Global under Grant N62909-19-1-2008. In addition, this research is partially supported by the Ministry of Science, Education and Technological Development of the Republic of Serbia grants OI174021, TR35004 and TR41008. The authors would like to express gratitude to company Saga New Frontier Group Belgrade, for supporting this research.

\bibliographystyle{elsarticle-num}
\bibliography{literatura}

\appendix

\section{FAIR-Bernoulli and FAIR-betaSF with baselines}
\label{app:baselines}

Baseline functions are a commonly used tool to reduce the variance of the estimate of the gradient in reinforcement learning algorithms. It is shown that introducing a baseline in loss function does not introduce additional bias into the model \cite{sutton2018reinforcement}. Here we explain how these techniques can be incorporated in FAIR models. These modifications are not included in experimental evaluation since the technique is already known and our main goal is to compare basic FAIR variants against existing baselines, but we still derive algorithms with baselines as they might be of practical importance.

Already discussed adversarial loss is augmented by adding another term -- the baseline loss:

\begin{equation}
{\cal L}_{\alpha}(\mu) = \mathrm{Var} \left[w\cdot \nabla_{\theta_g}\log P_g(w|\mathbf{x}) \cdot\left( \alpha \log P_\psi(\mathbf{s}|\mathbf{x}) - \log P_\phi(\mathbf{y}|\mathbf{x}) \right)
     - b_\mu(\mathbf{x}) \right]
\end{equation}
The baseline loss includes the gradient of the adversarial loss, since its purpose is to reduce the variance of the estimate of that gradient.
Keeping in mind that the variance can be represented as $\mathrm{Var}[x] = \mathbb{E}[x^2] - \mathbb{E}[x]^2$ (where squaring of a vector $v$ means $v^Tv$), the baseline loss can be simplified due to the fact that it holds
$\mathbb{E}_{P_\theta(w|\mathbf{x})}[\nabla_{\theta}\log P_\theta(w|\mathbf{x}) b_\mu(\mathbf{x})]=0$ \cite{sutton2018reinforcement}:

\begin{gather}
	\begin{split}
	&{\cal L}_{\alpha}(\mu) = \mathbb{E} \left[\left(w\cdot\nabla_{\theta}\log P_\theta(w|\mathbf{x})\cdot (\alpha \log P_\psi(\mathbf{s}|\mathbf{x}) - \log P_\phi(\mathbf{y}|\mathbf{x}))  - b_\mu(\mathbf{x})\right)^2 \right]
	\end{split}
\end{gather}
where the expectation is with respect to $(\mathbf{x},\mathbf{y},\mathbf{s})\sim P(\mathbf{x},\mathbf{y},\mathbf{s})$ and $w\sim P(w|\mathbf{x})$.
Furthermore, we assumed the independence among the values involved in the expectation, and thus the expectation can be represented as:
\begin{equation}
	\begin{split}
	{\cal L}_{\alpha}(\mu) = \mathbb{E} \left[\left(\nabla_{\theta}\log P_\theta(w|\mathbf{x})\right)^2 \right] \cdot \mathbb{E} \left[ \left(w \cdot (\alpha \log P_\psi(\mathbf{s}|\mathbf{x}) - \log P_\phi(\mathbf{y}|\mathbf{x})) - b_\mu(\mathbf{x})\right)^2 \right]
	\end{split}
\end{equation}

Considering that the first factor is constant with respect to $b_\mu$ it can be omitted, so that the final form of the loss ${\cal L}_{\alpha}(\mu)$  is:
\begin{equation}
\begin{split}
\mathbb{E} \left[\left(w \cdot (\alpha \log P_\psi(\mathbf{s}|\mathbf{x}) - \log P_\phi(\mathbf{y}|\mathbf{x})) - b_\mu(\mathbf{x})\right)^2\right]
\end{split}
\end{equation}

Then, the gradient $\nabla_\mu{\cal L}_\alpha(\mu)$ is:
\begin{equation}
\begin{split}
-\mathbb{E} \left[w \cdot \nabla_\mu b_\mu(\mathbf{x})\cdot\left(\alpha \log P_\psi(\mathbf{s}|\mathbf{x}) - \log P_\phi(\mathbf{y}|\mathbf{x}) - b_\mu(\mathbf{x})\right)\right]
\end{split}
\end{equation}

In Fig.~\ref{fig:Fig2} graphical representation of FAIR-betaSF with baseline is shown. The pseudo-code is presented in Algorithm~\ref{alg:beta-base}.

\begin{figure*}[h!]
	\center
	\includegraphics[width=1\textwidth]{Fig2.pdf}
	\captionsetup{justification=centering}
	\caption{Graphical representations of FAIR-betaSF model with baseline function}
	\label{fig:Fig2}
\end{figure*}

\begin{algorithm}[h!]
	\caption{FAIR-betaSF with baseline}
	\label{alg:beta-base}
	\begin{algorithmic}
		\State {\bfseries Input:} learning rates $\gamma_\theta, \gamma_\phi, \gamma_\psi$, $\gamma_b$ dataset $D$,  hyperparameter $\alpha$, number of iterations $M$
		\State {\bfseries Output:} parameters $\theta,\phi,\psi$, $\mu$
		\vspace{2mm}
		\State Initialize $\theta$, $\phi$, $\psi$, $\mu$
		\For{i = 1 to M}
		\State Sample a mini-batch $B\subseteq D$
		\State $\alpha_\mathbf{x},\beta_\mathbf{x}\leftarrow f_\theta(\mathbf{x})$ for each $\mathbf{x}\in B$
		\State Sample $w_\mathbf{x}\sim \beta(\alpha,\beta)$ for each $\mathbf{x}\in B$
		\State $d_{\theta} \leftarrow \gamma_\theta\frac{1}{|B|} \sum_{(\mathbf{x},\mathbf{y},\mathbf{s})\in B}\left[w_\mathbf{x}\nabla_{\theta}\log P_\theta(w_\mathbf{x}|\mathbf{x})\cdot\right.$
		
		\hspace{1.5cm}$\left.(\alpha\log P_\psi(\mathbf{s}|\mathbf{x})-\log P_\phi(\mathbf{y}|\mathbf{x})-b_\mu(\textbf{x}))\right]$
		\State $d_{\phi} \leftarrow \gamma_\phi\nabla_{\phi}{\cal L}^{\cal P}_\alpha(\theta,\phi,\psi,B)$
		\State $d_{\psi} \leftarrow -\gamma_\psi\nabla_{\psi}{\cal L}^{\cal P}_\alpha(\theta,\phi,\psi,B)$
		\State $d_{\mu} \leftarrow -\gamma_\mu\frac{1}{|B|} \sum_{(\mathbf{x},\mathbf{y},\mathbf{s})\in B}\left[w_\mathbf{x}\nabla_\mu b_\mu(\mathbf{x})\cdot\right.$
		
		\hspace{1.5cm}$\left.(\alpha\log P_\psi(\mathbf{s}|\mathbf{x})-\log P_\phi(\mathbf{y}|\mathbf{x}) - b_\mu(\mathbf{x}))\right]$
		\State $(\theta,\phi,\psi,\mu) \leftarrow (\theta,\phi,\psi,\mu) - (d_{\theta},d_{\phi}, d_{\psi},  d_{\mu})$
		\EndFor
	\end{algorithmic}
\end{algorithm}


\section{Model architecture}
\label{app:Architecture}

In all experiments with Reweighing - RF and DI - RF 500 trees were used in the random forest algorithm. Architectures, learning rates and maximum number of epochs used in models with neural networks for all datasets are presented in Tables~\ref{tab:A1},~\ref{tab:A11},~\ref{tab:A2},~\ref{tab:A21} and~\ref{tab:A3}.

% Table generated by Excel2LaTeX from sheet 'Arhitekture'
\begin{table*}
	\centering
	\caption{Architectures of models used}
	\label{Table:tab1}
	\begin{adjustbox}{max width=\textwidth, totalheight = \textheight-0.1in}
		\begin{tabular}{|L|L|L|L|L|L|}
			\toprule
			\textbf{Model} & {\textbf{No. of units per layer $P_\theta(w|\mathbf{x})$ or $P_\theta(\mathbf{z}|\mathbf{x})$}} & {\textbf{No. of units per layer $P_\phi(\mathbf{y}|\mathbf{x})$}} & {\textbf{No. of cells per layer $P_\psi(\mathbf{s}|\mathbf{x})$}} & \textbf{Activation} & {\textbf{Early stopping epoch / learning rate}} \\
			
			\midrule
			\multicolumn{6}{|c|}{\textbf{Adult}} \\
			\midrule
			\textbf{DI - NN} & - & 93/62/41/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{Reweighing - NN} & - & 93/62/41/27/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAD} & 93/62/41/27/18 & 18/12/1 & 18/12/1 & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-scalar} & 93/62/41/1 & 93/62/41/1 & 93/62/41/27/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaSF} & 93/62/41/1 & 93/62/41/27/1 & 93/62/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaREP} & 93/62/1 & 93/62/1 & 93/62/41/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-Bernoulli} & 93/62/1 & 93/62/41/1 & 93/62/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_diff} & - & 93/62/41/1 & -  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_non\_diff} & - & 93/62/1 & -  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{LURMI} & 93/62/41/27/18 & 18/12/8/1 & 18/12/8/5/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{CLFR} & 93/62/41/27/18 & 18/12/8/1 & 18/12/8/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\bottomrule
		\end{tabular}%
	\end{adjustbox}
	\label{tab:A1}%
\end{table*}%


\begin{table*}
	\centering
	\caption{Architectures of models used}
	\label{Table:tab1}
	\begin{adjustbox}{max width=\textwidth, totalheight = \textheight-0.1in}
		\begin{tabular}{|L|L|L|L|L|L|}
			\toprule
			\textbf{Model} & {\textbf{No. of units per layer $P_\theta(w|\mathbf{x})$ or $P_\theta(\mathbf{z}|\mathbf{x})$}} & {\textbf{No. of units per layer $P_\phi(\mathbf{y}|\mathbf{x})$}} & {\textbf{No. of units per layer $P_\psi(\mathbf{s}|\mathbf{x})$}} & \textbf{Activation} & {\textbf{Early stopping epoch / learning rate}} \\
			
			\midrule
			\multicolumn{6}{|c|}{\textbf{Readmission}} \\
			\midrule
			\textbf{DI - NN} & - &  929/619/412/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{Reweighing - NN} & - &  929/619/412/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAD} & 929/619/412 & 412/274/182/1 & 412/274/182/121/1 & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-scalar} & 929/619/1 & 929/619/412/1 & 929/619/412/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaSF} & 929/619/412/1 & 929/619/412/1 & 929/619/412/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaREP} & 929/619/412/274/1 & 929/619/412/1 & 929/619/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-Bernoulli} & 929/619/412/1 & 929/619/412/1 & 929/619/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_diff} & - & 929/619/412/274/182/1 & -  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_non\_diff} & - & 929/619/412/274/1 & -  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{LURMI} & 929/619/412/274 & 274/182/121/1 & 274/182/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{CLFR} & 929/619/412 & 412/274/1 & 412/274/182/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			
			\bottomrule
		\end{tabular}%
	\end{adjustbox}
	\label{tab:A11}%
\end{table*}%


\begin{table*}
	\centering
	\caption{Architectures of models used}
	\label{Table:tab1}
	\begin{adjustbox}{max width=\textwidth, totalheight = \textheight-0.1in}
		\begin{tabular}{|L|L|L|L|L|L|}
			\toprule
			\textbf{Model} & {\textbf{No. of units per layer $P_\theta(w|\mathbf{x})$ or $P_\theta(\mathbf{z}|\mathbf{x})$} } & {\textbf{No. of units per layer $P_\phi(\mathbf{y}|\mathbf{x})$}} & {\textbf{No. of cells per layer $P_\psi(\mathbf{s}|\mathbf{x})$}} & \textbf{Activation} & {\textbf{Early stopping epoch / learning rate}} \\
			
			\midrule
			\multicolumn{6}{|c|}{\textbf{Medical expenditures}} \\
			\midrule
			\textbf{DI - NN} & - & 137/91/60/40/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{Reweighing - NN} & - & 137/91/60/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAD} & 137/91/60 & 60/40/26/1 & 60/40/26/1 & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-scalar} & 137/91/1 & 137/91/60/40/1 & 137/91/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaSF} & 137/91/60/1 & 137/91/60/1 & 137/91/60/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaREP} & 137/91/1 & 137/91/60/40/1 & 137/91/60/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-Bernoulli} & 137/91/60/1 & 137/91/1 & 137/91/60/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_diff} & - & 137/91/60/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_non\_diff} & - & 137/91/60/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{LURMI} & 137/91/60 & 60/40/1 & 60/40/26/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{CLFR} & 137/91/60/40/26 & 26/17/1 & 26/17/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\bottomrule
		\end{tabular}%
	\end{adjustbox}
	\label{tab:A2}%
\end{table*}%

\begin{table*}
	\centering
	\caption{Architectures of models used}
	\label{Table:tab1}
	\begin{adjustbox}{max width=\textwidth, totalheight = \textheight-0.1in}
		\begin{tabular}{|L|L|L|L|L|L|}
			\toprule
			\textbf{Model} & {\textbf{No. of units per layer $P_\theta(w|\mathbf{x})$ or $P_\theta(\mathbf{z}|\mathbf{x})$} } & {\textbf{No. of units per layer $P_\phi(\mathbf{y}|\mathbf{x})$}} & {\textbf{No. of cells per layer $P_\psi(\mathbf{s}|\mathbf{x})$}} & \textbf{Activation} & {\textbf{Early stopping epoch / learning rate}} \\
			
			\midrule
			\multicolumn{6}{|c|}{\textbf{German credit - sex}} \\
			\midrule
			\textbf{DI - NN} & - & 56/37/24/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{Reweighing - NN} & - & 37/24/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAD} & 56/37/24/16 & 16/10/6/1 & 16/10/1 & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-scalar} & 56/37/1 & 56/37/24/1 & 56/37/24/16/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaSF} & 56/37/24/1 & 56/37/24/1 & 56/37/24/16/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaREP} & 56/37/24/16/1 & 56/37/24/1 & 56/37/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-Bernoulli} & 56/37/24/1 & 56/37/24/1 & 56/37/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_diff} & - & 56/37/24/16/1 & -  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_non\_diff} & - & 56/37/24/1 & -  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{LURMI} & 56/37/24 & 24/16/1 & 24/16/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{CLFR} & 56/37/24/16 & 16/10/6/1 & 16/10/6/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\bottomrule
		\end{tabular}%
	\end{adjustbox}
	\label{tab:A21}%
\end{table*}%


\begin{table*}
	\centering
	\caption{Architectures of models used}
	\label{Table:tab3}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{|L|L|L|L|L|L|}
			\toprule
			\textbf{Model} & {\textbf{No. of units per layer $P_\theta(w|\mathbf{x})$ or $P_\theta(\mathbf{z}|\mathbf{x})$} } & {\textbf{No. of units per layer $P_\phi(\mathbf{y}|\mathbf{x})$}} & {\textbf{No. of cells per layer $P_\psi(\mathbf{s}|\mathbf{x})$}} & \textbf{Activation} & {\textbf{Early stopping epoch / learning rate}} \\
			\midrule
			\multicolumn{6}{|c|}{\textbf{German credit - age}} \\
			\midrule
			\textbf{DI - NN} & - & 56/37/24/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{Reweighing - NN} & - & 56/37/24/1 & - & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAD} & 56/37/24/16 & 16/10/6/1 & 16/10/6/1 & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-scalar} & 56/37/24/1 & 56/37/24/16/1 & 56/37/24/16/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaSF} & 56/37/1 & 56/37/24/16/1 & 56/37/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-betaREP} & 56/37/24/16/1 & 56/37/24/1 & 56/37/1  & ReLU + Batch normalization  + sigmoid or exp (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{FAIR-Bernoulli} & 56/37/24/16/1 & 56/37/1 & 56/37/24/16/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_diff} & - & 56/24/16/1 & -  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{PYCO\_non\_diff} & - & 56/24/16/1 & -  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{LURMI} & 56/37/24/16 & 16/10/1 & 16/10/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\midrule
			\textbf{CLFR} & 56/37/24 & 24/16/10/1 & 24/16/1  & ReLU + Batch normalization  + sigmoid (last layer) & $15 / 10^{-4}$ \\
			\bottomrule
		\end{tabular}%
	\end{adjustbox}
	\label{tab:A3}%
\end{table*}%

\section{Additional results}
\label{app:Additional_results}

%Additional classification performance of all presented classifiers quantified by AUC and fairness metrics are presented in Fig.~\ref{fig:Adult1}, ~\ref{fig:Readmission1}, ~\ref{fig:MEPS191}, ~\ref{fig:Ger-sex-res1} and~\ref{fig:Ger-age-res1}.
More detailed results of experimental evaluation are given in Figs.~\ref{fig:Adult all-acc}, ~\ref{fig:Readmission all-acc},~\ref{fig:MEPS19 all-acc},~\ref{fig:Ger_age all-acc}, ~\ref{fig:Ger_sex all-acc}, which present Pareto fronts with all dominated and non-dominated models. 

Additionally, in Table~\ref{tab:Adult}, the Pareto optimal solutions obtained for all three fairness metrics and (ACC$_\mathbf{y}$) are presented for \textit{Adult income} dataset.It can be concluded that the number of FAIR models is larger compared to the other models and FAIR can therefore be considered better then other models. Simmilar conclusion can be confirmed for \textit{Hospital readmission} in the Table~\ref{tab:Readmission} where only FAIR models exist on overall Pareto front. In the case of \textit{Hospital expenditures} dataset, in Table~\ref{tab:MEPS-19} where overall Pareto front is presented, the FAIR models still dominate Pareto front. The same can be stated in the case of \textit{German credit} dataset, where overall Pareto fronts that are presented in Tables~\ref{tab:Ger-age} and~\ref{tab:Ger-sex} for age and sex sensitive attributes, respectively. 


\begin{table}
	\centering
	\caption{Pareto optimal solutions - \textit{Adult income} dataset}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{model}           & \textbf{ACC} & \textbf{AOD} & \textbf{ASD} & \textbf{AEOD} \\ \hline
		\textbf{PR}              & 0.7450       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{Reweighing-NN}   & 0.7927       & 0.0182       & 0.1517       & 0.0222        \\ \hline
		\textbf{Reweighing-NN}   & 0.8031       & 0.0200       & 0.1231       & 0.0052        \\ \hline
		\textbf{Reweighing-NN}   & 0.7963       & 0.0396       & 0.0918       & 0.1170        \\ \hline
		\textbf{FAIR-scalar}     & 0.8622       & 0.1151       & 0.0922       & 0.1544        \\ \hline
		\textbf{FAIR-scalar}     & 0.8050       & 0.0151       & 0.1105       & 0.0640        \\ \hline
		\textbf{FAIR-scalar}     & 0.8649       & 0.0939       & 0.1736       & 0.1236        \\ \hline
		\textbf{FAIR-scalar}     & 0.8630       & 0.0848       & 0.1672       & 0.1087        \\ \hline
		\textbf{FAIR-scalar}     & 0.7559       & 0.0090       & 0.0164       & 0.0161        \\ \hline
		\textbf{FAIR-scalar}     & 0.8110       & 0.0996       & 0.0654       & 0.1955        \\ \hline
		\textbf{FAIR-scalar}     & 0.8076       & 0.1466       & 0.0565       & 0.2841        \\ \hline
		\textbf{FAIR-betaSF}     & 0.8397       & 0.0607       & 0.1456       & 0.0627        \\ \hline
		\textbf{FAIR-betaSF}     & 0.8431       & 0.0551       & 0.1421       & 0.0676        \\ \hline
		\textbf{FAIR-betaREP}    & 0.8333       & 0.0550       & 0.1741       & 0.0390        \\ \hline
		\textbf{FAIR-betaREP}    & 0.8308       & 0.0475       & 0.1588       & 0.0287        \\ \hline
		\textbf{LURMI}           & 0.8212       & 0.0397       & 0.1420       & 0.0259        \\ \hline
		\textbf{LURMI}           & 0.7577       & 0.0157       & 0.0112       & 0.0286        \\ \hline
		\textbf{LURMI}           & 0.7653       & 0.0491       & 0.0860       & 0.0742        \\ \hline
		\textbf{PYCO\_diff}      & 0.8068       & 0.0984       & 0.0596       & 0.1917        \\ \hline
		\textbf{PYCO\_diff}      & 0.7810       & 0.0915       & 0.0233       & 0.1785        \\ \hline
		\textbf{PYCO\_diff}      & 0.8135       & 0.0824       & 0.1000       & 0.1490        \\ \hline
		\textbf{PYCO\_diff}      & 0.8162       & 0.1206       & 0.0780       & 0.2349        \\ \hline
		\textbf{PYCO\_diff}      & 0.8272       & 0.0925       & 0.1408       & 0.1487        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.7802       & 0.5744       & 0.0097       & 0.3087        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.7974       & 0.0828       & 0.0626       & 0.1496        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.8016       & 0.2105       & 0.0081       & 0.3506        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.8138       & 0.0707       & 0.1084       & 0.1199        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.8231       & 0.0836       & 0.1192       & 0.1408        \\ \hline
	\end{tabular}
	\label{tab:Adult}
\end{table}

\begin{table}
	\centering
	\caption{Pareto optimal solutions - \textit{Hospital readmission} dataset}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{model}          & \textbf{ACC} & \textbf{AOD} & \textbf{ASD} & \textbf{AEOD} \\ \hline
		\textbf{FAIR-scalar}    & 0.8559     & 0.0001     & 0.0000     & 0.0002      \\ \hline
		\textbf{FAIR-betaSF}    & 0.8654     & 0.0002     & 0.0008     & 0.0008      \\ \hline
		\textbf{FAIR-Bernoulli} & 0.8550     & 0.0000            & 0.0000            & 0.0000             \\ \hline
	\end{tabular}
	\label{tab:Readmission}%
\end{table}

\begin{table}
	\centering
	\caption{Pareto optimal solutions - \textit{Hospital expenditures} dataset}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{model}           & \textbf{ACC} & \textbf{AOD} & \textbf{ASD} & \textbf{AEOD} \\ \hline
		\textbf{PR}              & 0.8573       & 0.0334       & 0.0426       & 0.0589        \\ \hline
		\textbf{PR}              & 0.8291       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{DI-NN}           & 0.8459       & 0.0125       & 0.0324       & 0.0533        \\ \hline
		\textbf{Reweighing-NN}   & 0.8371       & 0.0107       & 0.0351       & 0.0078        \\ \hline
		\textbf{Reweighing-RF}   & 0.8644       & 0.0399       & 0.0653       & 0.0566        \\ \hline
		\textbf{FAIR-scalar}     & 0.8703       & 0.0574       & 0.0844       & 0.0801        \\ \hline
		\textbf{FAIR-scalar}     & 0.8657       & 0.0562       & 0.0837       & 0.0755        \\ \hline
		\textbf{FAIR-scalar}     & 0.8522       & 0.0276       & 0.0205       & 0.0512        \\ \hline
		\textbf{FAIR-scalar}     & 0.8484       & 0.0426       & 0.0099       & 0.0816        \\ \hline
		\textbf{FAIR-betaSF}     & 0.8615       & 0.0030       & 0.0502       & 0.0127        \\ \hline
		\textbf{FAIR-betaSF}     & 0.8636       & 0.0130       & 0.0520       & 0.0075        \\ \hline
		\textbf{FAIR-betaSF}     & 0.8611       & 0.0115       & 0.0545       & 0.0021        \\ \hline
		\textbf{FAIR-Bernoulli}  & 0.8648       & 0.0560       & 0.0866       & 0.0705        \\ \hline
		\textbf{FAIR-betaREP}    & 0.8623       & 0.0116       & 0.0534       & 0.0024        \\ \hline
		\textbf{FAIR-betaREP}    & 0.8619       & 0.0067       & 0.0519       & 0.0076        \\ \hline
		\textbf{FAIR-betaREP}    & 0.8581       & 0.0110       & 0.0556       & 0.0027        \\ \hline
		\textbf{FAIR-betaREP}    & 0.8564       & 0.0099       & 0.0575       & 0.0035        \\ \hline
		\textbf{PYCO\_diff}      & 0.8488       & 0.0845       & 0.0063       & 0.1556        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.8319       & 0.0446       & 0.0160       & 0.0785        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.8463       & 0.0541       & 0.0014       & 0.0983        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.8661       & 0.0265       & 0.1038       & 0.0025        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.8640       & 0.0335       & 0.0839       & 0.0344        \\ \hline
	\end{tabular}
	\label{tab:MEPS-19}%
\end{table}


\begin{table}
	\centering
	\caption{Pareto optimal solutions - \textit{German credit} - age dataset}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{model}          & \textbf{ACC} & \textbf{AOD} & \textbf{ASD} & \textbf{AEOD} \\ \hline
		\textbf{DI-NN}          & 0.7800       & 0.0254       & 0.0537       & 0.1000        \\ \hline
		\textbf{DI-NN}          & 0.7700       & 0.0421       & 0.0427       & 0.1333        \\ \hline
		\textbf{DI-NN}          & 0.7700       & 0.0471       & 0.0134       & 0.0333        \\ \hline
		\textbf{DI-RF}          & 0.6900       & 0.0167       & 0.0110       & 0.0333        \\ \hline
		\textbf{DI-RF}          & 0.6900       & 0.0167       & 0.0110       & 0.0333        \\ \hline
		\textbf{DI-RF}          & 0.6900       & 0.0167       & 0.0110       & 0.0333        \\ \hline
		\textbf{Reweighing-NN}  & 0.7900       & 0.0410       & 0.1087       & 0.0000        \\ \hline
		\textbf{Reweighing-RF}  & 0.7000       & 0.0333       & 0.0220       & 0.0667        \\ \hline
		\textbf{FAIR-scalar}    & 0.6800       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{FAIR-scalar}    & 0.7500       & 0.0008       & 0.0867       & 0.1000        \\ \hline
		\textbf{FAIR-scalar}    & 0.7800       & 0.0852       & 0.1026       & 0.0333        \\ \hline
		\textbf{FAIR-betaSF}    & 0.6800       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{FAIR-Bernoulli} & 0.7600       & 0.2260       & 0.0024       & 0.5667        \\ \hline
		\textbf{FAIR-betaREP}   & 0.6900       & 0.0167       & 0.0110       & 0.0333        \\ \hline
		\textbf{FAIR-betaREP}   & 0.7800       & 0.1977       & 0.0366       & 0.4000        \\ \hline
		\textbf{PYCO\_diff}     & 0.7400       & 0.0224       & 0.0410       & 0.0278        \\ \hline
		\textbf{PYCO\_diff}     & 0.7067       & 3.4655       & 0.0117       & 0.2444        \\ \hline
		\textbf{PYCO\_diff}     & 0.7067       & 0.0251       & 0.0269       & 0.0211        \\ \hline
	\end{tabular}
	\label{tab:Ger-age}
\end{table}

\begin{table}
	\centering
	\caption{Pareto optimal solutions - \textit{German credit} - sex dataset}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{model}           & \textbf{ACC} & \textbf{AOD} & \textbf{ASD} & \textbf{AEOD} \\ \hline
		\textbf{PR}              & 0.6800       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{DI-RF}           & 0.7000       & 0.0167       & 0.0233       & 0.0333        \\ \hline
		\textbf{DI-RF}           & 0.7000       & 0.0178       & 0.0041       & 0.0167        \\ \hline
		\textbf{Reweighing-NN}   & 0.7300       & 0.0017       & 0.0208       & 0.1167        \\ \hline
		\textbf{Reweighing-RF}   & 0.7000       & 0.0167       & 0.0233       & 0.0333        \\ \hline
		\textbf{Reweighing-RF}   & 0.7800       & 0.0184       & 0.0771       & 0.0833        \\ \hline
		\textbf{Reweighing-RF}   & 0.7500       & 0.0289       & 0.0715       & 0.0667        \\ \hline
		\textbf{FAD}             & 0.7100       & 0.0154       & 0.0046       & 0.1667        \\ \hline
		\textbf{FAIR-scalar}     & 0.7600       & 0.0300       & 0.0619       & 0.1167        \\ \hline
		\textbf{FAIR-scalar}     & 0.7467       & 0.3124       & 0.0114       & 0.4643        \\ \hline
		\textbf{FAIR-betaSF}     & 0.6800       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{FAIR-betaSF}     & 0.8000       & 0.0050       & 0.0812       & 0.0000        \\ \hline
		\textbf{FAIR-betaREP}    & 0.7000       & 0.0101       & 0.0112       & 0.1333        \\ \hline
		\textbf{FAIR-betaREP}    & 0.6900       & 0.0006       & 0.0025       & 0.1333        \\ \hline
		\textbf{FAIR-betaREP}    & 0.8100       & 0.2228       & 0.2704       & 0.3500        \\ \hline
		\textbf{CLFR}            & 0.6600       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{CLFR}            & 0.6600       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{CLFR}            & 0.6600       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{CLFR}            & 0.6600       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{LURMI}           & 0.7000       & 0.0151       & 0.0210       & 0.0960        \\ \hline
		\textbf{LURMI}           & 0.6600       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{LURMI}           & 0.7000       & 0.0273       & 0.0094       & 0.0152        \\ \hline
		\textbf{PYCO\_diff}      & 0.6600       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.6600       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.6600       & 0.0000       & 0.0000       & 0.0000        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.7133       & 0.0334       & 0.0022       & 0.0502        \\ \hline
		\textbf{PYCO\_non\_diff} & 0.7533       & 0.1361       & 0.0251       & 0.2083        \\ \hline
	\end{tabular}
	\label{tab:Ger-sex}%
\end{table}

\iffalse
\begin{figure*}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{Adult.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Adult income} datasets}
	\label{fig:Adult1}
	\vskip -0.2in
\end{figure*}

\begin{figure}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{Readmission.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Hospital readmission} dataset}
	\label{fig:Readmission1}
	\vskip -0.2in
\end{figure}

\begin{figure}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{MEPS19.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Hospital expenditures} dataset}
	\label{fig:MEPS191}
	\vskip -0.2in
\end{figure}

\begin{figure}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{Ger_age.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{German credit} (age) dataset}
	\label{fig:Ger-sex-res1}
	\vskip -0.2in
\end{figure}

\begin{figure}
	\center
	\includegraphics[angle=0, width=0.7\textwidth]{Ger_sex.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{German credit} (sex) dataset}
	\label{fig:Ger-age-res1}
	\vskip -0.2in
\end{figure}
\fi


\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{Adult_all_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Adult income} dataset}
	\label{fig:Adult all-acc}
	\vskip -0.2in
\end{figure*}

\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{Readmission_all_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Hospital readmission} dataset}
	\label{fig:Readmission all-acc}
	\vskip -0.2in
\end{figure*}

\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{MEPS19_all_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Hospital expenditures} dataset}
	\label{fig:MEPS19 all-acc}
	\vskip -0.2in
\end{figure*}

\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{Ger_age_all_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{German credit} (age) dataset}
	\label{fig:Ger_age all-acc}
	\vskip -0.2in
\end{figure*}

\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{Ger_sex_all_ACC.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by ACC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{German credit} (sex) dataset}
	\label{fig:Ger_sex all-acc}
	\vskip -0.2in
\end{figure*}



\iffalse
\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{Adult_all.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Adult income} dataset}
	\label{fig:Adult all}
	\vskip -0.2in
\end{figure*}

\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{Readmission_all.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Hospital readmission} dataset}
	\label{fig:Readmission all}
	\vskip -0.2in
\end{figure*}

\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{MEPS19_all.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{Hospital expenditures} dataset}
	\label{fig:MEPS19 all}
	\vskip -0.2in
\end{figure*}

\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{Ger_age_all.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{German credit} (age) dataset}
	\label{fig:Ger_age all}
	\vskip -0.2in
\end{figure*}

\begin{figure*}
	\center
	\includegraphics[angle=0, width=1\textwidth]{Ger_sex_all.png}
	\captionsetup{justification=centering}
	\caption{Classification performance and fairness of models as measured by AUC$_\mathbf{y}$ and AOD, ASD, or AEOD on the \textit{German credit} (sex) dataset}
	\label{fig:Ger_sex all}
	\vskip -0.2in
\end{figure*}
\fi
%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix



\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
